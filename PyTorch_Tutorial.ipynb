{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPUae5S+V1J/qxlF4KaHN7R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Introduction\n",
        "- From video: https://www.youtube.com/watch?v=OIenNRt2bjg&list=LL&index=2"
      ],
      "metadata": {
        "id": "j5-1T4LFo_7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Torch Basic Syntax"
      ],
      "metadata": {
        "id": "TwZlfmf2MGyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# empty 2x2x3 tensor\n",
        "x = torch.empty(2, 2, 3)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_9UU8ZApBjl",
        "outputId": "f6f4c087-772c-4f77-de59-9aa4b58e99aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[6.1633e-32, 0.0000e+00, 0.0000e+00],\n",
            "         [0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "        [[6.0988e-38, 6.2815e-38, 0.0000e+00],\n",
            "         [0.0000e+00, 7.4056e-37, 4.1703e-42]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5x3 tensor with random values\n",
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "K3jsvEUTpE5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86edad11-a38b-4b7c-b89b-c8966e424b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7602, 0.0658, 0.1487],\n",
            "        [0.1816, 0.8014, 0.9906],\n",
            "        [0.3217, 0.0724, 0.4876],\n",
            "        [0.5805, 0.5373, 0.5290],\n",
            "        [0.4587, 0.6754, 0.7414]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros(5, 3)\n",
        "print(x)\n",
        "# can also do torch.ones"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REvUj_GbHG6j",
        "outputId": "b4d17ccc-7ecd-4811-e187-f343bb6466d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "# can also do this\n",
        "print(x.size())\n",
        "# to get size of first dimension (number of rows)\n",
        "print(x.size(0))\n",
        "# to get size of second dimension (number of columns)\n",
        "print(x.size(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLHbjvKvHT29",
        "outputId": "f6437355-9b5d-44c6-cf49-03f71a2dd963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "torch.Size([5, 3])\n",
            "5\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62chjRYmHYnZ",
        "outputId": "5f5a0dd2-fa03-4216-8ba0-8764634b1b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct torch tensor from an array\n",
        "x = torch.tensor([2., 4., 3.])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "602AzbesH8_A",
        "outputId": "4a1d857b-ecd8-444a-a968-14c24286c939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor argument requires_grad=True tells pytorch that it needs to calculate gradients which is needed for optimization\n",
        "# by default it is false so need to explicitly set to true\n",
        "# for requires_grad to be applicable data type needs to be float not int\n",
        "x = torch.tensor([2.0, 4.0, 3.0], requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVnXXa92ILh9",
        "outputId": "50946087-baee-4bb0-e682-b7fce2400ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 4., 3.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(2, 2)\n",
        "y = torch.rand(2, 2)\n",
        "# elementwise addition\n",
        "z = x + y\n",
        "# can also do z = torch.add(x, y)\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)\n",
        "\n",
        "# or in place addition\n",
        "z.add_(x)\n",
        "print(z)\n",
        "\n",
        "# multiplication elementwise\n",
        "z = x * y\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHLvr3jOIcOF",
        "outputId": "5c61f684-f046-4e9c-c294-f9f1fa968b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0.5830, 0.0762],\n",
            "        [0.6074, 0.1773]])\n",
            "tensor([[1.5830, 1.0762],\n",
            "        [1.6074, 1.1773]])\n",
            "tensor([[2.5830, 2.0762],\n",
            "        [2.6074, 2.1773]])\n",
            "tensor([[0.5830, 0.0762],\n",
            "        [0.6074, 0.1773]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# slicing works like numpy\n",
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "print(x[:,0]) # print full column 0\n",
        "print(x[1,:]) # print full row 1\n",
        "print(x[1, 1]) # element access, but it returns a tensor with the single value\n",
        "# to get the item itself append .item()\n",
        "print(x[1, 1].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwlyRXzyIvqy",
        "outputId": "188212d6-e961-4b83-b668-6df97a0cac2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3936, 0.4779, 0.2744],\n",
            "        [0.9081, 0.6503, 0.1617],\n",
            "        [0.0140, 0.8471, 0.8964],\n",
            "        [0.7152, 0.4342, 0.3703],\n",
            "        [0.7504, 0.1638, 0.4709]])\n",
            "tensor([0.3936, 0.9081, 0.0140, 0.7152, 0.7504])\n",
            "tensor([0.9081, 0.6503, 0.1617])\n",
            "tensor(0.6503)\n",
            "0.6503470540046692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to numpy\n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "b = a.numpy()\n",
        "print(b)\n",
        "print(type(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng7faqqpKCAJ",
        "outputId": "0f4c4979-c56f-4414-d0b5-6584587ca942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "[1. 1. 1. 1. 1.]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# note that if the tensor is on the CPU, then both objects share same memory\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X97U4uDRKnWZ",
        "outputId": "95728154-bafd-4f86-b052-dcfb0080f8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert from numpy to torch\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a) #from_numpy will share same memory as a\n",
        "c = torch.tensor(a) #tensor will create a separate copy\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f48TVQH-K8m4",
        "outputId": "49adffcc-ade0-4a6c-8d01-7bed8c1c8f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU support\n",
        "# tensors by default created on CPU, but can also move them to GPU or create them on GPU directly\n",
        "device = torch.device('cude' if torch.cude.is_available() else 'cpu')\n",
        "\n",
        "x = torch.rand(2,2).to(device) # first creates on CPU, and then moves to GPU if available\n",
        "# or move to CPU or GPU\n",
        "x = x.to('cpu')\n",
        "x = x.to('cuda')\n",
        "\n",
        "# or create directly on GPU\n",
        "x = torch.rand(2, 2, device=device)"
      ],
      "metadata": {
        "id": "d4hVvwP0LHSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograd\n",
        "- Autograd package provides automatic differentiation for operations on tensors\n",
        "- Compute partial derivatives while applying chain rule\n",
        "- Must set requires_grad=True\n",
        "- PyTorch uses a computation graph to track the sequence of operations applied to tensors during forward pass in order to compute gradients during .backward()\n",
        "- Any manual operations performed on the tensors outside of the \"learning logic\" should not be recorded\n",
        "- For instance, after computing the gradients, the weights are manually updated. This operation should not be tracked on the computation graph because a) increases memory usage, b) breaks the gradient logic, c) slows training\n",
        "- So when updating weights, must set requires_grad=False before, and then set back to requires_grad=True after"
      ],
      "metadata": {
        "id": "l8MyLaVGMKhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x + 2\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(y.grad_fn)\n",
        "# y has an attribute grad_fn which is set to addbackward\n",
        "# since y is an addition function it stores what the function is for the backpropagation\n",
        "# also note that since y is a function of x, y also has requires_grad as true\n",
        "print(y.requires_grad)\n",
        "\n",
        "\n",
        "z = y ** 2\n",
        "print(z)\n",
        "print(z.grad_fn)\n",
        "# compute the mean of the tensor\n",
        "z = z.mean()\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VISW2v_aMLlb",
        "outputId": "8e5f4291-5ea6-48a0-9552-269af3378809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7214, 0.7776, 0.3249], requires_grad=True)\n",
            "tensor([2.7214, 2.7776, 2.3249], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7fce25b49720>\n",
            "True\n",
            "tensor([7.4060, 7.7150, 5.4051], grad_fn=<PowBackward0>)\n",
            "<PowBackward0 object at 0x7fce25b49720>\n",
            "tensor(6.8420, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to compute gradients with backpropagation, call .backward() and the gradients compute automatically\n",
        "# the gradients will be accumulated into the .grad attribute, which is the partial derivative of the function wrt the tensor\n",
        "print(x.grad)\n",
        "\n",
        "# backpropagate\n",
        "# note this only works if z is a scalar, not a tensor\n",
        "# for instance if I didn't do z.mean(), it would give an error\n",
        "z.backward()\n",
        "print(x.grad)\n",
        "# IMPORTANT NOTE: .backward() ACCUMULATES into the .grad attribute\n",
        "# so if in a loop the .grad isn't cleared, it will keep adding\n",
        "# make sure to do optimizer.zero_grad() to reset gradients every loop\n",
        "# or do:\n",
        "x.grad.zero_()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjDXjlfmN85d",
        "outputId": "5f03c0f6-dbfd-4ccf-e21b-ea09aa668256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([1.8143, 1.8517, 1.5499])\n",
            "tensor([0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sometimes we want to stop a tensor from tracking the gradient history, ex) after training during evaluation, or when manually updating the weights\n",
        "- Use x.requires_grad_(False)\n",
        "- Or use x.detach()\n",
        "- Or wrap in \"with torch.no_grad():\""
      ],
      "metadata": {
        "id": "UOu33jFyQRIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using .requires_grad_(False)\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "b = (a * a).sum()\n",
        "print(a.requires_grad)\n",
        "print(b.grad_fn)\n",
        "\n",
        "a.requires_grad_(False)\n",
        "b = (a * a).sum()\n",
        "print(a.requires_grad)\n",
        "print(b.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igKejIIsQlq3",
        "outputId": "cadd2591-0b71-40fe-b922-bd359d9b3892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "<SumBackward0 object at 0x7fce25b27160>\n",
            "False\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# detach creates a new copy tensor with requires_grad false\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "b = a.detach()\n",
        "print(a.requires_grad)\n",
        "print(b.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBlCFR9TQtgi",
        "outputId": "0950d379-2937-49dc-9492-f3c6ae090075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrap in with torch.no_grad():\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(a.requires_grad)\n",
        "b = a ** 2\n",
        "print(b.requires_grad)\n",
        "with torch.no_grad():\n",
        "  b = a ** 2\n",
        "  print(b.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-aiTT9pRHUE",
        "outputId": "d4b6c8bb-a2d1-4e65-bdaf-a29867a41da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent in Autograd\n",
        "- Linear regression\n",
        "- Weight times input plus bias\n",
        "- $ f(x) = w * x + b$\n",
        "- Want to approximate $f(x) = 2 * x$"
      ],
      "metadata": {
        "id": "BK4C0aDaTBt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# training samples, 1 input 1 output, 8 samples\n",
        "X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8], dtype=torch.float32)\n",
        "# y values are 2x\n",
        "Y = torch.tensor([2, 4, 6, 8, 10, 12, 14, 16], dtype=torch.float32)\n",
        "\n",
        "# weight initialized to zero\n",
        "# requires_grad because during backpropagation we need to compute gradient of loss wrt w\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "def loss(y, y_pred):\n",
        "  return ((y_pred - y)**2).mean()\n",
        "\n",
        "X_test = 5.0\n",
        "print(f\"Prediction before training: f({X_test}) = {forward(X_test).item():.3f}\")\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "  # calculate predictions using forward pass\n",
        "  y_pred = forward(X)\n",
        "  # compute the loss\n",
        "  l = loss(Y, y_pred)\n",
        "  # calculate the gradients wrt weight\n",
        "  l.backward()\n",
        "\n",
        "  # during the update of the weights we don't want to track these operations\n",
        "  # so wrap in with torch.no_grad() so the operations aren't added to computation graph\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # remember to reset the gradients in the weight .grad attribute\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w.item():.3f}, loss= {l.item():.3f}')\n",
        "\n",
        "# correctly predicts the output\n",
        "print(f\"Prediction after training: f({X_test}) = {forward(X_test).item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSXuc2UbTGVq",
        "outputId": "b3fce925-5032-494e-b5b5-b4b798659f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5.0) = 0.000\n",
            "epoch 10: w = 1.998, loss= 0.000\n",
            "epoch 20: w = 2.000, loss= 0.000\n",
            "epoch 30: w = 2.000, loss= 0.000\n",
            "epoch 40: w = 2.000, loss= 0.000\n",
            "epoch 50: w = 2.000, loss= 0.000\n",
            "epoch 60: w = 2.000, loss= 0.000\n",
            "epoch 70: w = 2.000, loss= 0.000\n",
            "epoch 80: w = 2.000, loss= 0.000\n",
            "epoch 90: w = 2.000, loss= 0.000\n",
            "epoch 100: w = 2.000, loss= 0.000\n",
            "Prediction after training: f(5.0) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model, Loss and Optimizer\n",
        "- Typical PyTorch pipeline:\n",
        "1. Design model (input, output, forward pass with different layers)\n",
        "2. Construct loss and optimizer\n",
        "3. Training loop with forward and backward propagation"
      ],
      "metadata": {
        "id": "W2oa9k41YWGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural network for a linear regression model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# torch neural network object expects tensors in a certain shape\n",
        "# this is like a one column 8 row matrix\n",
        "X = torch.tensor([[1],[2],[3],[4],[5],[6],[7],[8]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8],[10],[12],[14],[16]], dtype=torch.float32)\n",
        "\n",
        "# number of samples is number of rows, number of features is columns\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_test = torch.tensor([10], dtype=torch.float32)\n",
        "\n",
        "# neural network classes that you define must inherit from the nn.Module\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    # call the init of the nn parent\n",
        "    super().__init__()\n",
        "    # define different layers\n",
        "    # for linear regression we add only one Linear layer\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # returns the predictions\n",
        "    return self.lin(x)\n",
        "\n",
        "# 1. create model\n",
        "input_size, output_size = n_features, n_features\n",
        "model = LinearRegression(input_size, output_size)\n",
        "print(f\"Prediction before training: f({X_test}) = {model(X_test).item():.3f}\")\n",
        "\n",
        "# 2. define the loss and optimizer\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "# mean square error loss\n",
        "loss = nn.MSELoss()\n",
        "# SGD optimizer\n",
        "# optimizer always gets model.parameters() which are the weights and bias parameters\n",
        "# learning rate optional hyperparameter\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 3. define training loop\n",
        "for epoch in range(n_epochs):\n",
        "  # when model is called, PyTorch automatically triggers a __call__ method which calls the forward class\n",
        "  # so no need to call model.forward()\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # calculate loss, pass in the expected then predicted\n",
        "  l = loss(Y, y_pred)\n",
        "  # calculate gradients\n",
        "  l.backward()\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    w, b = model.parameters() # returns weights and biases\n",
        "    print(f'epoch {epoch+1}, w = {w[0][0].item()}, loss = {l.item()}')\n",
        "\n",
        "print(f\"Prediction after training: f({X_test.item()}) = {model(X_test).item():.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_ZcXM5LYYhx",
        "outputId": "ca1b0254-af8a-45b0-94cc-498fac9da9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(tensor([10.])) = -2.488\n",
            "epoch 10, w = 2.086758852005005, loss = 0.0510685071349144\n",
            "epoch 20, w = 2.084592819213867, loss = 0.046957120299339294\n",
            "epoch 30, w = 2.0812761783599854, loss = 0.04334663599729538\n",
            "epoch 40, w = 2.0780892372131348, loss = 0.04001369699835777\n",
            "epoch 50, w = 2.0750269889831543, loss = 0.03693711757659912\n",
            "epoch 60, w = 2.072084903717041, loss = 0.034096989780664444\n",
            "epoch 70, w = 2.069258213043213, loss = 0.031475286930799484\n",
            "epoch 80, w = 2.066542387008667, loss = 0.02905518375337124\n",
            "epoch 90, w = 2.0639328956604004, loss = 0.026821179315447807\n",
            "epoch 100, w = 2.0614259243011475, loss = 0.02475883439183235\n",
            "Prediction after training: f(10.0) = 20.269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Neural Network\n",
        "- Follows same pipeline as the single neuron\n"
      ],
      "metadata": {
        "id": "1f6l6KUTUmTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training a NN to recognize digits 0-9\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# configure the device for GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define the hyperparameters\n",
        "input_size = 784 # images of shape 28x28\n",
        "hidden_size = 500 # number of neurons in hidden layer\n",
        "num_classes = 10 # 10 because 10 digits from 0-9\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset built in from torch\n",
        "# this returns set of 600000 images\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                            train=True,\n",
        "                                            transform=transforms.ToTensor(),\n",
        "                                            download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                            train=True,\n",
        "                                            transform=transforms.ToTensor())\n",
        "\n",
        "# data loader built in from torch\n",
        "# provides optimized way to iterate over the dataset\n",
        "# since batch_size was defined to be 100, the length of train_loader is 600 instead of 600000 (600 batches of 100)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "# example of the images\n",
        "examples = iter(test_loader)\n",
        "example_data, example_targets = next(examples)\n",
        "\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(example_data[i][0], cmap='gray')\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "# create the network model\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size) # this is the weighted sum plus biases, if bias=True\n",
        "    self.relu = nn.ReLU() # the activation function for layer 1\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    # no activation applied at output. PyTroch crossentropy requires values with no activation applied\n",
        "    return out\n",
        "\n",
        "# to use the GPU, the model needs to be pushed to the device\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# define loss and optimizer\n",
        "# crossentropy used for classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# define the training loop\n",
        "# have 2 epochs (number of iterations over the entire training set)\n",
        "# each epoch, define a loop to train on each batch in train_loader\n",
        "# each batch contain 100 images. So model will perform forward/backward pass on a batch of 100, update params, move to next batch of 100\n",
        "# and repeat 600 times to look at 600000 images\n",
        "# then repeat for another epoch\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  # unpack each batch of 100 images and labels (expected output)\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # the images need to be reshaped into the shape the NN expects\n",
        "    # the original shape is a 4D matrix of size [100, 1, 28, 28]\n",
        "    # the NN expects [100, 784] where there are 100 rows (number of images in batch) and 784 pixels per image\n",
        "    # since the model was pushed to the GPU, the tensors must also be pushed to GPU\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # forward pass by calling model with the inputs\n",
        "    outputs = model(images)\n",
        "    # average loss in the batch of 100 images\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # calculate gradients\n",
        "    loss.backward()\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    # reset the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Step [{i+1}/{n_total_steps}], Loss: {loss}')\n",
        "\n",
        "# evaluate the performance of the model\n",
        "# all model evaluation should be done in torch.no_grad to prevent gradient tracking\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = len(test_loader.dataset)\n",
        "\n",
        "  for images, labels in test_loader:\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "\n",
        "    # outputs is the raw output\n",
        "    # need to find the max value per image, and its index which indicates the prediction\n",
        "    # torch.max returns the maximum output value and its index\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  acc = n_correct / n_samples\n",
        "  print(f'Accuracy of the network on the {n_samples} test images: {acc*100} %')\n",
        "\n",
        "PATH = './digit_id.path'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "CqeOmISEnAEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1903e33-72bb-4ee5-a18d-c34fa1658d83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Step [100/600], Loss: 0.2334097921848297\n",
            "Epoch 1/2, Step [200/600], Loss: 0.398881733417511\n",
            "Epoch 1/2, Step [300/600], Loss: 0.22981984913349152\n",
            "Epoch 1/2, Step [400/600], Loss: 0.16696451604366302\n",
            "Epoch 1/2, Step [500/600], Loss: 0.1656610369682312\n",
            "Epoch 1/2, Step [600/600], Loss: 0.22521807253360748\n",
            "Epoch 2/2, Step [100/600], Loss: 0.06800297647714615\n",
            "Epoch 2/2, Step [200/600], Loss: 0.11497944593429565\n",
            "Epoch 2/2, Step [300/600], Loss: 0.06505492329597473\n",
            "Epoch 2/2, Step [400/600], Loss: 0.09721935540437698\n",
            "Epoch 2/2, Step [500/600], Loss: 0.04298058897256851\n",
            "Epoch 2/2, Step [600/600], Loss: 0.06278544664382935\n",
            "Accuracy of the network on the 60000 test images: 97.82499999999999 %\n"
          ]
        }
      ]
    }
  ]
}